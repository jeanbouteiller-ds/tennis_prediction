{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeanbouteiller-ds/tennis_prediction/blob/main/Players_fixed_table.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gorg-TjzXeal",
        "outputId": "aef50d60-6bf2-4e50-fd80-f360b343ccaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.7.22)\n",
            "Collecting PyGithub\n",
            "  Downloading PyGithub-2.1.1-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pynacl>=1.4.0 (from PyGithub)\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from PyGithub) (2.8.2)\n",
            "Requirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (2.31.0)\n",
            "Collecting pyjwt[crypto]>=2.4.0 (from PyGithub)\n",
            "  Downloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (4.5.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (2.0.6)\n",
            "Collecting Deprecated (from PyGithub)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (41.0.4)\n",
            "Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pynacl>=1.4.0->PyGithub) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.14.0->PyGithub) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.14.0->PyGithub) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.14.0->PyGithub) (2023.7.22)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from Deprecated->PyGithub) (1.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->PyGithub) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.4.1->pynacl>=1.4.0->PyGithub) (2.21)\n",
            "Installing collected packages: pyjwt, Deprecated, pynacl, PyGithub\n",
            "  Attempting uninstall: pyjwt\n",
            "    Found existing installation: PyJWT 2.3.0\n",
            "    Uninstalling PyJWT-2.3.0:\n",
            "      Successfully uninstalled PyJWT-2.3.0\n",
            "Successfully installed Deprecated-1.2.14 PyGithub-2.1.1 pyjwt-2.8.0 pynacl-1.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install requests\n",
        "!pip install PyGithub  # Install the PyGitHub library"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import random\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "import os\n",
        "from github import Github\n",
        "from io import StringIO\n"
      ],
      "metadata": {
        "id": "4uiY8VTA6kN2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Set Up Github Environment"
      ],
      "metadata": {
        "id": "t_8rkjbj_xFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Replace these with your GitHub credentials\n",
        "# username = \"jeanbouteiller-ds\"\n",
        "\n",
        "# github_url = \"https://raw.githubusercontent.com/jeanbouteiller-ds/tennis_prediction/main/password.txt\"\n",
        "# response = requests.get(github_url)\n",
        "\n",
        "# password= response.text.replace('\\n','')\n",
        "# repository_name = \"tennis_prediction\"\n",
        "# branch_name = \"main\"  # Replace with your branch name\n",
        "\n",
        "\n",
        "# # Initialize the GitHub client\n",
        "# g = Github(username, password)\n",
        "\n",
        "# # Access the desired repository\n",
        "# repo = g.get_user().get_repo(repository_name)\n",
        "\n",
        "\n",
        "# def add_file_github(file_name,file_content):\n",
        "#   try:\n",
        "#       file = repo.get_contents(file_name, ref=branch_name)\n",
        "#       sha = file.sha\n",
        "#   except Exception as e:\n",
        "#       sha = None\n",
        "\n",
        "#   if sha:\n",
        "#       repo.update_file(file_name, \"Update file\", file_content, sha, branch_name)\n",
        "#       # print(f\"Updated {file_name} in {repository_name}\")\n",
        "#   else:\n",
        "#       repo.create_file(file_name, \"Create file\", file_content, branch_name)\n",
        "#       # print(f\"Created {file_name} in {repository_name}\")\n",
        "\n",
        "# def get_file_content(file_name):\n",
        "#   file_content = repo.get_contents(file_name, ref=branch_name)\n",
        "#   file_content = file_content.decoded_content.decode(\"utf-8\")\n",
        "#   if file_name.split('.')[1]=='txt':\n",
        "#     file_content=eval(file_content)\n",
        "#     return(file_content)\n",
        "#   if file_name.split('.')[1]=='csv':\n",
        "#     file_content=pd.read_csv(StringIO(file_content))\n",
        "#     return(file_content)"
      ],
      "metadata": {
        "id": "KnltnHuj_0Fy"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ3tMiFua2jH"
      },
      "source": [
        "# Get the list of all players and their associated urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "xdDpI62ia13V"
      },
      "outputs": [],
      "source": [
        "# #get the list of all first days of week between 2 dates\n",
        "# #needed since the url on the atp website is updated every monday\n",
        "# def get_list_ranking_dates(start_date, end_date):\n",
        "#     mondays = []\n",
        "#     current_date = start_date\n",
        "#     one_day = timedelta(days=1)\n",
        "#     while current_date <= end_date:\n",
        "#         if current_date.weekday() == 0:  # Monday has a weekday index of 0\n",
        "#             mondays.append(current_date.strftime('%Y-%m-%d'))\n",
        "#         current_date += one_day\n",
        "#     return mondays\n",
        "\n",
        "# #get the list of all urls in which we have the rankings\n",
        "# def find_list_weekly_ranking_url(start_date, end_date):\n",
        "#   list_ranking_urls=[]\n",
        "#   #date should be in format YYYY-MM-DD and should be a string\n",
        "#   list_dates=get_list_ranking_dates(start_date, end_date)\n",
        "\n",
        "#   for date_ranking in list_dates:\n",
        "#     list_ranking_urls.append('https://www.atptour.com/en/rankings/singles?rankRange=1-200&rankDate='+date_ranking)\n",
        "#   return(list_ranking_urls)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "eHvcYNfZxX6T"
      },
      "outputs": [],
      "source": [
        "# #for each url, we extract the spans with the url associated with all players\n",
        "# def span_from_url(weekly_ranking_url,span_list):\n",
        "#   headers = {\n",
        "#       'User-Agent': 'Your User Agent String Here'+str(random.random())\n",
        "#   }\n",
        "\n",
        "#   # Send an HTTP GET request with headers\n",
        "#   response = requests.get(weekly_ranking_url, headers=headers)\n",
        "\n",
        "#   # Check if the request was successful (status code 200)\n",
        "#   if response.status_code == 200:\n",
        "#       # Get the HTML content from the response\n",
        "#       html_code = response.text\n",
        "#   else:\n",
        "#       html_code=''\n",
        "\n",
        "#   soup = BeautifulSoup(html_code, 'html.parser')\n",
        "#   selected_span={}\n",
        "\n",
        "#   for span_name in span_list:\n",
        "#     selected_span[span_name] = soup.find_all('span', class_=span_name)\n",
        "\n",
        "#   return (selected_span)\n",
        "\n",
        "# #from the above spans, we extract all urls\n",
        "# def players_from_url(weekly_ranking_url):\n",
        "\n",
        "#   selected_span=span_from_url(weekly_ranking_url,['player-cell-wrapper'])['player-cell-wrapper']\n",
        "\n",
        "#   list_urls=[]\n",
        "#   list_players=[]\n",
        "\n",
        "#   for span_element in selected_span:\n",
        "#     a_element = span_element.find('a')\n",
        "\n",
        "#       # Extract the href attribute\n",
        "#     if a_element:\n",
        "#       # print(a_element)\n",
        "#       href = 'https://www.atptour.com'+a_element.get('href')\n",
        "#       player_name=href.split('players')[1].split('/')[1]\n",
        "#       list_players.append(player_name)\n",
        "#       list_urls.append(href)\n",
        "#     else:\n",
        "#         print(\"No <a> element found within the <span>.\")\n",
        "#   return(list_players,list_urls)\n",
        "\n",
        "# #\n",
        "# def update_names_and_urls(weekly_ranking_url,list_all_names,list_all_urls):\n",
        "#   list_players_name_and_url=players_from_url(weekly_ranking_url)\n",
        "#   set_names_week = set(list_players_name_and_url[0])\n",
        "#   set_urls_week = set(list_players_name_and_url[1])\n",
        "#   set_names_all = set(list_all_names)\n",
        "#   set_urls_all = set(list_all_urls)\n",
        "\n",
        "#   names_to_add=list(set_names_week-set_names_all)\n",
        "#   urls_to_add=list(set_urls_week-set_urls_all)\n",
        "\n",
        "#   return(list_all_names+names_to_add,\n",
        "#          list_all_urls+urls_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PCwNHwCbVHv",
        "outputId": "865e1c2a-a4dc-4b7e-f9ea-58e1141ae173"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['2000']\n",
            "['2000', '2001']\n",
            "['2000', '2001', '2002']\n",
            "['2000', '2001', '2002', '2003']\n",
            "['2000', '2001', '2002', '2003', '2004']\n",
            "['2000', '2001', '2002', '2003', '2004', '2005']\n",
            "['2000', '2001', '2002', '2003', '2004', '2005', '2006']\n",
            "['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007']\n",
            "['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008']\n",
            "['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009']\n",
            "['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010']\n",
            "['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011']\n",
            "['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012']\n",
            "['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013']\n",
            "['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014']\n",
            "['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015']\n",
            "['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016']\n",
            "['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017']\n",
            "['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018']\n",
            "['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019']\n",
            "['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020']\n",
            "['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021']\n",
            "['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022']\n",
            "['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023']\n"
          ]
        }
      ],
      "source": [
        "# list_all_names=[]\n",
        "# list_all_urls=[]\n",
        "\n",
        "# # Define your start and end dates\n",
        "# start_date = datetime(2000, 1, 1)\n",
        "# end_date = datetime(2023, 10, 5)  # Change this to your desired end date\n",
        "\n",
        "# list_year=[]\n",
        "# for weekly_url_ranking in find_list_weekly_ranking_url(start_date, end_date):\n",
        "#   if weekly_url_ranking.split('rankDate=')[1].split('-')[0] not in list_year:\n",
        "#     list_year.append(weekly_url_ranking.split('rankDate=')[1].split('-')[0])\n",
        "#     print(list_year)\n",
        "#   names_and_url_week=update_names_and_urls(weekly_url_ranking,list_all_names,list_all_urls)\n",
        "#   list_all_names=names_and_url_week[0]\n",
        "#   list_all_urls=names_and_url_week[1]\n",
        "\n",
        "# list_all_urls_final=[0 for k in range(len(list_all_urls))]\n",
        "# for k in range(len(list_all_names)):\n",
        "#   player_name=list_all_names[k]\n",
        "#   for player_url in list_all_urls:\n",
        "#     if player_name==player_url.split('/players/')[1].split('/')[0]:\n",
        "#       list_all_urls_final[k]=player_url\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # # Create a new file with content\n",
        "# file_name_player_names = \"list_all_players_names.txt\"  # The name of the new file\n",
        "# file_name_urls = \"list_all_urls.txt\"  # The name of the new file\n",
        "# list_all_names_str=str(list_all_names)\n",
        "# list_all_urls_str=str(list_all_urls_final)\n",
        "\n",
        "# add_file_github(file_name_player_names,list_all_names_str)\n",
        "# add_file_github(file_name_urls,list_all_urls_str)"
      ],
      "metadata": {
        "id": "-WEm8lKM2B8p"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Players Data"
      ],
      "metadata": {
        "id": "4YLTQ9nAyZnC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "rslHQS1EXrKv"
      },
      "outputs": [],
      "source": [
        "# def find_stats_from_span(url,span_names_list=['table-height-cm-wrapper','table-weight-lbs','table-birthday'],\n",
        "#                feature_names=['height','weight (lbs)','Birthdate']):\n",
        "#   dict_data={}\n",
        "#   data_spans=span_from_url(url,span_names_list)\n",
        "#   for k in range (len(feature_names)):\n",
        "#     key=feature_names[k]\n",
        "#     values=list(data_spans.values())[k][0].get_text()\n",
        "#     dict_data[key]=values.replace(' ','').replace('\\r\\n','')\n",
        "#   return dict_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def find_stats_from_div(url,div_dict={'Turned Pro':['table-big-label','table-big-value'],\n",
        "#                                       'Plays':['table-label','table-value']}):\n",
        "#   #div_dict should be a dict where the key is the label that we want to find in the html code\n",
        "#   # and the values are the div that we want to search (for label and values)\n",
        "#   dict_data={}\n",
        "#   headers = {\n",
        "#       'User-Agent': 'Your User Agent String Here'+str(random.random())\n",
        "#   }\n",
        "\n",
        "#   # Send an HTTP GET request with headers\n",
        "#   response = requests.get(url, headers=headers)\n",
        "\n",
        "#   # Check if the request was successful (status code 200)\n",
        "#   if response.status_code == 200:\n",
        "#       # Get the HTML content from the response\n",
        "#       html_code = response.text\n",
        "#   else:\n",
        "#       html_code=''\n",
        "\n",
        "#   soup = BeautifulSoup(html_code, 'html.parser')\n",
        "\n",
        "#   for feature_name in div_dict.keys():\n",
        "\n",
        "#     div_name=div_dict[feature_name][0]\n",
        "#     div_value=div_dict[feature_name][1]\n",
        "\n",
        "#     for div in soup.find_all('div', class_=div_name):\n",
        "#       # print(div)\n",
        "#       if feature_name in div.get_text():\n",
        "\n",
        "#         dict_data[feature_name]=(div.find_next_sibling('div', class_=div_value).get_text()).replace(' ','').replace('\\r\\n','')\n",
        "#         # if\n",
        "#         # print(div.find_next_sibling('div', class_='table-big-value').get_text())\n",
        "\n",
        "#   return (dict_data)\n",
        "\n",
        "\n",
        "# def find_all_stats(url):\n",
        "#   return({**find_stats_from_div(url),**find_stats_from_span(url)})"
      ],
      "metadata": {
        "id": "axoxENTs6eZ5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# list_player_names=get_file_content('list_all_players_names.txt')\n",
        "# list_urls=get_file_content('list_all_urls.txt')\n",
        "\n",
        "# import pandas as pd\n",
        "# df_player_fixed=pd.DataFrame()\n",
        "# nb_error=0\n",
        "# for player_k in range(len(list_player_names)):\n",
        "#   url=list_urls[player_k]\n",
        "#   player_name=list_player_names[player_k]\n",
        "#   try:\n",
        "#     df_player_fixed[player_name]=find_all_stats(url)\n",
        "#   except Exception as e:\n",
        "#     nb_error+=1\n",
        "\n",
        "# csv_player_fixed=df_player_fixed.T.to_csv(index=True)\n",
        "\n",
        "# add_file_github('df_player_fixed.csv',csv_player_fixed)"
      ],
      "metadata": {
        "id": "qHKHEis671g8"
      },
      "execution_count": 24,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1eKWlpyRPemghzmN8X1cYFW6MOJTJpUDm",
      "authorship_tag": "ABX9TyNzOPh2/bHkSKjcWDk5GrvC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
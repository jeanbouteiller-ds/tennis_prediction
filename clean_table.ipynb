{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNWLYz8OE0nh8PWM+SKQaLt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeanbouteiller-ds/tennis_prediction/blob/main/clean_table.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_package(package_name):\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package_name])\n",
        "        print(f\"Successfully installed {package_name}\")\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(f\"Failed to install {package_name}\") # Install the PyGitHub library\n",
        "\n",
        "install_package(\"requests\")\n",
        "install_package(\"PyGitHub\")\n",
        "install_package(\"pandas\")\n",
        "install_package(\"tqdm\")"
      ],
      "metadata": {
        "id": "zBnT4oD2GAI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 271,
      "metadata": {
        "id": "k7w9-KyhTAtX"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import io\n",
        "import importlib.util\n",
        "import sys\n",
        "import nbformat\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Define the URL of the nb1 notebook on GitHub\n",
        "github_url = \"https://raw.githubusercontent.com/jeanbouteiller-ds/tennis_prediction/main/functions/functions_fixed_table.ipynb\"\n",
        "\n",
        "# Download the notebook as a raw .ipynb file\n",
        "response = requests.get(github_url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "  notebook_content = response.text\n",
        "\n",
        "  # Parse the notebook content\n",
        "  notebook = nbformat.reads(notebook_content, as_version=4)\n",
        "\n",
        "  # Now you can execute the cells in the notebook\n",
        "  for cell in notebook.cells:\n",
        "      if cell.cell_type == 'code':\n",
        "          exec(cell.source)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_matchs=get_file_content('data/df_all_matchs.csv')\n",
        "df_ranking=get_file_content('data/df_all_ranking.csv')\n",
        "df_betting=get_file_content('data/df_all_betting.csv')\n",
        "df_fixed_table=get_file_content('data/df_player_fixed.csv')\n",
        "df_tournaments=get_file_content('data/df_tournaments.csv')"
      ],
      "metadata": {
        "id": "97tsMuUrZXSb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31cb90ca-0bc6-47f5-b197-2b5ecc432a66"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<string>:49: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "<string>:49: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions Used in the script"
      ],
      "metadata": {
        "id": "lXjIIP2c9l1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_unique_items(l1, l2):\n",
        "    # Convert the lists to sets for efficient set operations\n",
        "    set1 = set(l1)\n",
        "    set2 = set(l2)\n",
        "\n",
        "    # Calculate the items unique to each list\n",
        "    unique_in_l1 = list(set1.difference(set2))\n",
        "    unique_in_l2 = list(set2.difference(set1))\n",
        "\n",
        "    return unique_in_l1, unique_in_l2\n",
        "\n",
        "def add_one_year(date_string):\n",
        "  #add one year tpo a date fprmat 2000.01.01 as a string\n",
        "  return(datetime.strftime(datetime.strptime(date_string,\"%Y.%m.%d\")+timedelta(days=365),\"%Y.%m.%d\"))\n",
        "\n",
        "from dateutil import parser\n",
        "\n",
        "def convert_date_format(input_date):\n",
        "    try:\n",
        "        date_obj = parser.parse(input_date)\n",
        "        formatted_date = date_obj.strftime(\"%Y.%m.%d\")\n",
        "        return formatted_date\n",
        "    except ValueError:\n",
        "        return \"Invalid Date Format\"\n"
      ],
      "metadata": {
        "id": "Vi6NLH51iYFn"
      },
      "execution_count": 265,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_location(tournament_name):\n",
        "  dict_hard_coding={\"'\":\"\",\"chi minh\":\"chi min\",\"napoli\":\"naples\",\"new york city\":\"new york\",\"-\":\" \",\"queens club\":\"london\",\n",
        "                    \"atp finals\":\"london\"}\n",
        "  tournament_name=tournament_name.split(',')[0].lower()\n",
        "  for key in dict_hard_coding.keys():\n",
        "    tournament_name=tournament_name.replace(key,dict_hard_coding[key])\n",
        "  return tournament_name.split(',')[0].lower().strip()\n",
        "\n",
        "def create_tournament_unique_id(df,tournament_name_col,tournament_date_col):\n",
        "  df['unique_tournament_id']=df[tournament_name_col].apply(lambda x:clean_location(x))+'//'+df[tournament_date_col].apply(lambda x:convert_date_format(x))\n",
        "  return df"
      ],
      "metadata": {
        "id": "XqjPCQDN9lMF"
      },
      "execution_count": 264,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Speficic Dataset Cleaning"
      ],
      "metadata": {
        "id": "wlA8mEg6Ugon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## All Matchs Cleaning"
      ],
      "metadata": {
        "id": "WMRM0FasTEyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_matchs_clean=df_matchs.copy()\n",
        "\n",
        "#We make sure to remove duplicates\n",
        "df_matchs_clean=df_matchs_clean.dropna(subset=['loser_name','winner_name'])\n",
        "\n",
        "#some rounds do not make sense\n",
        "list_rounds_to_remove=['International Jr Event','Wheelchair','Olympic Bronze','Champions Tour','3rd/4th Place Match']\n",
        "df_matchs_clean=df_matchs_clean[~df_matchs_clean['round'].isin(list_rounds_to_remove)]\n",
        "\n",
        "#make sure that we consider only data until today\n",
        "df_matchs_clean=df_matchs_clean[pd.to_datetime(df_matchs_clean['tournament_date'])<=datetime.today()]\n",
        "\n",
        "#remove games with \"Bye\" - means that the player did not play the game before\n",
        "df_matchs_clean=df_matchs_clean[df_matchs_clean['loser_name']!='Bye']\n",
        "\n",
        "#We can then see that we can remove games with no score\n",
        "df_matchs_clean=df_matchs_clean[~df_matchs_clean['score'].isna()]\n",
        "\n",
        "#We will remove games with games in 4 points\n",
        "def is_correct_score(score):\n",
        "  score_list=score.split('-')\n",
        "  if score in [\"(W/O)\",\"(RET)\",\"(DEF)\"]:\n",
        "    return True\n",
        "  if '(RET)' in score or \"DEF\" in score:\n",
        "    return True\n",
        "  for set_ in score_list:\n",
        "    if int(set_[0])<6 and int(set_[1])<6 and len(score_list)>1:\n",
        "      # print('false returned',set_)\n",
        "      return False\n",
        "    if int(set_[0])>=6 or int(set_[1])>=6:\n",
        "      return True\n",
        "  return \"?\"\n",
        "\n",
        "df_false=df_matchs_clean[df_matchs_clean['score'].apply(lambda x:is_correct_score(x))==False]\n",
        "tournament_to_remove=df_false.drop_duplicates(['tournament_name','tournament_date'])\n",
        "tournament_to_remove=tournament_to_remove['tournament_name']+'-'+tournament_to_remove['tournament_date'].values\n",
        "\n",
        "df_matchs_clean=df_matchs_clean[~(df_matchs_clean['tournament_name']+'-'+df_matchs_clean['tournament_date']).isin(tournament_to_remove)]\n",
        "\n",
        "#remove matchs that are duplicates (same players, tournament, score & round)\n",
        "#this is because the scraping does not understand when the tournament has been canceled\n",
        "#Only 10 games in tennis history have the same players, tournament, etc. We will consider that these will not bias the model\n",
        "columns_to_check=['tournament_name','loser_name','winner_name','score','round']\n",
        "df_matchs_clean=df_matchs_clean.sort_values('tournament_date')\n",
        "df_matchs_clean = df_matchs_clean[~df_matchs_clean.duplicated(subset=columns_to_check, keep='first')]\n",
        "\n",
        "\n",
        "#removing tournaments that have round robin for only a few games\n",
        "rr=df_matchs_clean[df_matchs_clean['round']=='Round Robin']\n",
        "remove_round_robin=((rr['tournament_name']+'-'+rr['tournament_date']).value_counts()<5)[((rr['tournament_name']+'-'+rr['tournament_date']).value_counts()<5)==True].index.values\n",
        "df_matchs_clean=df_matchs_clean[~((df_matchs_clean['tournament_name']+'-'+df_matchs_clean['tournament_date']).isin(remove_round_robin))]\n",
        "\n",
        "#Removal of players that have never lost\n",
        "never_lost_players=find_unique_items(df_matchs_clean['winner_name'],df_matchs_clean['loser_name'])[0]\n",
        "df_matchs_clean=df_matchs_clean[~df_matchs_clean['winner_name'].isin(never_lost_players)]\n",
        "\n",
        "#Feature cleaning: round has to uniformized\n",
        "df_matchs_clean['round']=df_matchs_clean['round'].replace({'Finals':'Final','Quarterfinals':'Quarter-Finals','Semifinals':'Semi-Finals'})\n",
        "\n",
        "#Feature cleaning: remove capital letters\n",
        "df_matchs_clean['winner_name']=df_matchs_clean['winner_name'].str.lower()\n",
        "df_matchs_clean['loser_name']=df_matchs_clean['loser_name'].str.lower()\n",
        "\n",
        "#Feature cleaning: we use the tournaments table to find the name of the tournament that are missing\n",
        "df_tournaments_nan=df_tournaments[df_tournaments['tournament_name'].isna()].copy()\n",
        "df_tournaments_nan['tournament_name']=df_tournaments_nan['url'].apply(lambda x:x.split('/')[3])\n",
        "\n",
        "df_matchs_clean.loc[df_matchs_clean['tournament_name'].isna(),'tournament_name']=df_matchs_clean[df_matchs_clean['tournament_name'].isna()].merge(df_tournaments_nan,on=['tournament_date'])['tournament_name_y'].values\n",
        "\n",
        "#remove tournaments that are played in a team competition (that biases the model)\n",
        "#we consider that tournaments in a team are those for which we have many finals\n",
        "df_final=df_matchs_clean[df_matchs_clean['round']==\"Final\"] #this needs to happen after round cleaning\n",
        "columns_to_check=['tournament_name','tournament_date']\n",
        "df_final = df_final[df_final.duplicated(subset=columns_to_check, keep=False)]\n",
        "list_team_event=(df_final['tournament_name']+'-'+df_final['tournament_date']).unique()\n",
        "df_matchs_clean=df_matchs_clean[~((df_matchs_clean['tournament_name']+'-'+df_matchs_clean['tournament_date']).isin(list_team_event))]\n",
        "\n",
        "#Create a tournament unique id\n",
        "df_matchs_clean=create_tournament_unique_id(df_matchs_clean,'tournament_name','tournament_date')\n",
        "\n",
        "#After all this cleaning, we remove duplicates\n",
        "df_matchs_clean=df_matchs_clean.drop_duplicates()\n"
      ],
      "metadata": {
        "id": "zOhQUpqJTG1E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4efee107-4237-46c2-e110-fcd4a7a0b763"
      },
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-266-7465f4d525a1>:58: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_matchs_clean['round']=df_matchs_clean['round'].replace({'Finals':'Final','Quarterfinals':'Quarter-Finals','Semifinals':'Semi-Finals'})\n",
            "<ipython-input-266-7465f4d525a1>:61: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_matchs_clean['winner_name']=df_matchs_clean['winner_name'].str.lower()\n",
            "<ipython-input-266-7465f4d525a1>:62: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_matchs_clean['loser_name']=df_matchs_clean['loser_name'].str.lower()\n",
            "<ipython-input-266-7465f4d525a1>:68: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_matchs_clean.loc[df_matchs_clean['tournament_name'].isna(),'tournament_name']=df_matchs_clean[df_matchs_clean['tournament_name'].isna()].merge(df_tournaments_nan,on=['tournament_date'])['tournament_name_y'].values\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ranking Cleaning"
      ],
      "metadata": {
        "id": "BnBeqJyFXRrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_ranking_clean=df_ranking.copy()\n",
        "df_ranking_clean=df_ranking_clean.drop_duplicates()\n",
        "\n",
        "#clean the ranking, we have sometimes \"65T\" as a ranking this means that 2 players are ex aequo. We will simply remove the T.\n",
        "df_ranking_clean['ranking']=df_ranking_clean['ranking'].astype(str)\n",
        "df_ranking_clean['ranking']=df_ranking_clean['ranking'].str.replace('T','')\n",
        "\n",
        "#cleaning of the player_name field\n",
        "df_ranking_clean['player_name']=df_ranking_clean['player_name'].str.lower()\n",
        "df_ranking_clean['player_name']=df_ranking_clean['player_name'].str.replace('  ',' ')\n",
        "\n",
        "#Note:players in df_ranking not in df_matchs are those who never played an atp 250 game or more\n",
        "#Note: players if df_matchs and not in df_ranking are those who never got better than 350 in the world"
      ],
      "metadata": {
        "id": "tTQcr31pWbEi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tournaments Cleaning"
      ],
      "metadata": {
        "id": "8__7TRcZVDiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_tournaments_clean=df_tournaments.copy()\n",
        "\n",
        "#make sure that we consider only data until today\n",
        "df_tournaments_clean=df_tournaments_clean[pd.to_datetime(df_tournaments_clean['tournament_date'])<=datetime.today()]\n",
        "\n",
        "#remove tournaments with same name & date\n",
        "df_tournaments_clean=df_tournaments_clean.drop_duplicates(['tournament_name','tournament_date'])\n",
        "\n",
        "#remove doubles tournaments\n",
        "df_tournaments_clean=df_tournaments_clean[~df_tournaments_clean['url'].str.contains('double')]\n",
        "\n",
        "#clean tournament_name null\n",
        "df_tournaments_clean.loc[df_tournaments_clean['tournament_name'].isna(),'tournament_name']=df_tournaments_clean.loc[df_tournaments_clean['tournament_name'].isna(),'url'].apply(lambda x: x.split('/')[3])\n",
        "\n",
        "#add a few tournaments that are missing in atp\n",
        "tournament_name_date_dict={'Bastad, Sweden':['2011.07.11'],\n",
        "                           'Nur-Sultan, Kazakhstan':['2021.09.20','2022.09.20']}\n",
        "\n",
        "for tournament_name in tournament_name_date_dict.keys():\n",
        "  list_dates=tournament_name_date_dict[tournament_name]\n",
        "  for date_str in list_dates:\n",
        "    row_year_before=df_tournaments_clean[(df_tournaments_clean['tournament_name']==tournament_name)&\n",
        "                                      (df_tournaments_clean['tournament_date']==date_str)]\n",
        "    row_year_before['tournament_date']= add_one_year(row_year_before['tournament_date'].values[0])\n",
        "    df_tournaments_clean=df_tournaments_clean.append(row_year_before)\n",
        "\n",
        "#create a tournament_unique_id\n",
        "df_tournaments_clean=create_tournament_unique_id(df_tournaments_clean,'tournament_name','tournament_date')\n",
        "\n",
        "df_tournaments_clean=df_tournaments_clean.sort_values('tournament_date')"
      ],
      "metadata": {
        "id": "ThTOeA8yZGrL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a1ade66-0716-4a3b-aa25-4eb7a9ced883"
      },
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-272-093a8345ba3d>:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  row_year_before['tournament_date']= add_one_year(row_year_before['tournament_date'].values[0])\n",
            "<ipython-input-272-093a8345ba3d>:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df_tournaments_clean=df_tournaments_clean.append(row_year_before)\n",
            "<ipython-input-272-093a8345ba3d>:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  row_year_before['tournament_date']= add_one_year(row_year_before['tournament_date'].values[0])\n",
            "<ipython-input-272-093a8345ba3d>:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df_tournaments_clean=df_tournaments_clean.append(row_year_before)\n",
            "<ipython-input-272-093a8345ba3d>:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  row_year_before['tournament_date']= add_one_year(row_year_before['tournament_date'].values[0])\n",
            "<ipython-input-272-093a8345ba3d>:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df_tournaments_clean=df_tournaments_clean.append(row_year_before)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fixed Table Cleaning"
      ],
      "metadata": {
        "id": "oN8MTwMEZ-ML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_fixed_table_clean=df_fixed_table.copy()\n",
        "\n",
        "import re\n",
        "import math\n",
        "\n",
        "def extract_height(height_string):\n",
        "    # Define a regular expression pattern to match the height format \"xxxcm\"\n",
        "    pattern = r'(\\d+\\.\\d+|\\d+)cm'\n",
        "\n",
        "    # Use re.search to find the height value in the input string\n",
        "    match = re.search(pattern, height_string)\n",
        "\n",
        "    # Check if a match was found\n",
        "    if match:\n",
        "        # Extract the matched value and convert it to a float\n",
        "        height_value = float(match.group(1))\n",
        "        return height_value\n",
        "    else:\n",
        "        # Return NaN (Not-a-Number) for invalid input\n",
        "        return math.nan\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "def extract_birthdate(date_string):\n",
        "    possible_formats = [\"%Y.%m.%d\", \"%Y-%m-%d\", \"%Y/%m/%d\", \"%Y%m%d\", \"%Y %m %d\", \"(%Y.%m.%d)\", \"(%Y-%m-%d)\", \"(%Y/%m/%d)\", \"(%Y%m%d)\", \"(%Y %m %d)\"]\n",
        "\n",
        "    for date_format in possible_formats:\n",
        "        try:\n",
        "            # Remove parentheses if present and then parse the date\n",
        "            date_string = date_string.strip('()')\n",
        "            birthdate = datetime.strptime(date_string, date_format)\n",
        "            return birthdate.date()\n",
        "        except ValueError:\n",
        "            continue  # Try the next format\n",
        "\n",
        "    # If none of the formats match, handle the error\n",
        "    return None  # or raise an exception if preferred\n",
        "\n",
        "df_fixed_table_clean['Favorite_Hand']=df_fixed_table['Plays'].apply(lambda x:x.split('-Handed')[0])\n",
        "df_fixed_table_clean['BackHand']=df_fixed_table['Plays'].apply(lambda x:x.split(',')[1].split('Backhand')[0])\n",
        "df_fixed_table_clean['Height (CM)']=df_fixed_table['height'].apply(lambda x:extract_height(x))\n",
        "df_fixed_table_clean['Birthdate']=df_fixed_table_clean['Birthdate'].apply(lambda x:extract_birthdate(x))\n",
        "\n",
        "df_fixed_table_clean=df_fixed_table_clean.drop(['Plays','height'],axis=1)"
      ],
      "metadata": {
        "id": "jpnwPbS5Z4NB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Betting Cleaning"
      ],
      "metadata": {
        "id": "CPOhaXIXWW9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "df_betting_clean=df_betting.copy()\n",
        "\n",
        "columns_no_odds=['Location','Tournament','Date','Winner','Loser']\n",
        "columns_odds=[k for k in df_betting.columns if k not in columns_no_odds]\n",
        "df_odds=df_betting[columns_odds]\n",
        "\n",
        "\n",
        "#We remove games for which we have no odds\n",
        "df_betting_clean=df_betting_clean[~(df_odds.isna().sum(axis=1)==len(columns_odds))]\n",
        "\n",
        "#Cleaning of Player Names\n",
        "df_betting_clean[\"Loser\"]=df_betting_clean[\"Loser\"].str.strip().str.lower()\n",
        "df_betting_clean[\"Winner\"]=df_betting_clean[\"Winner\"].str.strip().str.lower()\n",
        "\n",
        "#If we have AvgW and AvgL\n",
        "df_betting_clean.loc[~df_betting_clean['AvgW'].isna(),'Final_odd_winner']=df_betting_clean.loc[~df_betting_clean['AvgW'].isna(),'AvgW']\n",
        "df_betting_clean.loc[~df_betting_clean['AvgL'].isna(),'Final_odd_loser']=df_betting_clean.loc[~df_betting_clean['AvgL'].isna(),'AvgL']\n",
        "\n",
        "# Create 'Final_odd_winner' and 'Final_odd_loser' based on the conditions\n",
        "def calculate_final_odds(row):\n",
        "  win_columns = [col for col in row.index[5:] if col.endswith('W') and not col.startswith('MaxW')]\n",
        "  lose_columns = [col for col in row.index[5:] if col.endswith('L') and not col.startswith('MaxL')]\n",
        "  # print(row,win_columns,row[win_columns].mean(skipna=True))\n",
        "\n",
        "  final_odd_winner = row[win_columns].mean(skipna=True)\n",
        "  final_odd_loser = row[lose_columns].mean(skipna=True)\n",
        "\n",
        "  return final_odd_winner, final_odd_loser\n",
        "\n",
        "df_betting_clean.loc[df_betting_clean['Final_odd_winner'].isna(),['Final_odd_winner', 'Final_odd_loser']] = df_betting_clean[df_betting_clean['Final_odd_winner'].isna()].apply(lambda row: pd.Series(calculate_final_odds(row)), axis=1).values\n",
        "\n",
        "# Print the DataFrame with the new columns\n",
        "df_betting_clean=df_betting_clean.drop(columns_odds,axis=1)"
      ],
      "metadata": {
        "id": "W-8ELzOkS4jh"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Date corresponds to the game_date, but we need to round it to the first game recorded\n",
        "def find_tournament_month(tournament_date):\n",
        "  if int(tournament_date.split('-')[0])==12 and int(tournament_date.split('-')[2])>25:\n",
        "    return str(int(tournament_date.split('-')[0])+1)+'-'+'01'\n",
        "  return str(int(tournament_date.split('-')[0]))+'-'+ tournament_date.split('-')[1]\n",
        "\n",
        "df_betting_clean['month']=df_betting_clean['Date'].apply(lambda x:find_tournament_month(x))\n",
        "df_betting_clean['tournament_Date']=df_betting_clean.merge(df_betting_clean.groupby([\"Location\",\"month\"]).min()['Date'].reset_index(),\n",
        "                       on=['Location','month'],how=\"left\")['Date_y'].values\n",
        "df_betting_clean['tournament_Date']=df_betting_clean['tournament_Date'].apply(lambda x:convert_date_format(x))\n",
        "\n",
        "#create a tournament_unique_id\n",
        "df_betting_clean=create_tournament_unique_id(df_betting_clean,'Location','tournament_Date')\n",
        "\n",
        "#manual mapping for master tournament & a few_time\n",
        "list_master=['sydney//2001.11.12','shanghai//2002.11.11','houston//2003.11.10','houston//2004.11.15',\n",
        "             'shanghai//2005.11.13','shanghai//2006.11.12','shanghai//2007.11.11','shanghai//2008.11.09']\n",
        "dict_mapping_master={}\n",
        "for k in list_master:\n",
        "  dict_mapping_master[k]=\"tennis masters cup//\"+k.split('//')[1]\n",
        "\n",
        "dict_mapping_master['vienna//2003.11.12']='vienna//2003.10.06'\n",
        "dict_mapping_master['miami//2017.01.02']='miami//2017.03.22'\n",
        "\n",
        "df_betting_clean.loc[df_betting_clean['unique_tournament_id'].isin(list(dict_mapping_master.keys())),'unique_tournament_id']=df_betting_clean.loc[df_betting_clean['unique_tournament_id'].isin(list(dict_mapping_master.keys())),'unique_tournament_id'].map(dict_mapping_master)\n",
        "\n",
        "#We look for tournaments in df_betting that have no map\n",
        "list_unique_ids_tourn=df_tournaments_clean['unique_tournament_id'].unique()\n",
        "list_unique_ids_bet=df_betting_clean['unique_tournament_id'].unique()\n",
        "no_match=(find_unique_items(list_unique_ids_bet,list_unique_ids_tourn)[0])\n",
        "\n",
        "#sometimes whe have a few days differences, we will consider them\n",
        "def change_unique_id_no_match(unique_id,day_change):\n",
        "  date_wrong=datetime.strptime(unique_id.split('//')[1],\"%Y.%m.%d\")\n",
        "  date_minus_1=(datetime.strftime(date_wrong+timedelta(day_change),\"%Y.%m.%d\"))\n",
        "  return unique_id.split('//')[0]+'//'+date_minus_1\n",
        "\n",
        "dict_mapping={}\n",
        "day_change_list=sorted([k for k in range(-20,20,1)],key=abs)\n",
        "for k in no_match:\n",
        "  for day_change in day_change_list:\n",
        "    if change_unique_id_no_match(k,day_change) in list_unique_ids_tourn:\n",
        "      dict_mapping[k]=change_unique_id_no_match(k,day_change)\n",
        "      break\n",
        "\n",
        "df_betting_clean.loc[(df_betting_clean['unique_tournament_id'].isin(no_match)&\n",
        "                     df_betting_clean['unique_tournament_id'].isin(list(dict_mapping.keys()))),'unique_tournament_id']=df_betting_clean.loc[(df_betting_clean['unique_tournament_id'].isin(no_match)&\n",
        "                                                                                                                                     df_betting_clean['unique_tournament_id'].isin(list(dict_mapping.keys()))),'unique_tournament_id'].map(dict_mapping)\n",
        "list_unique_ids_bet2=df_betting_clean['unique_tournament_id'].unique()\n",
        "no_match2=(find_unique_items(list_unique_ids_bet2,list_unique_ids_tourn)[0])"
      ],
      "metadata": {
        "id": "djQC9NgcLgQw"
      },
      "execution_count": 273,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# String Cleaning"
      ],
      "metadata": {
        "id": "_a7LYvF2QVox"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Player Names"
      ],
      "metadata": {
        "id": "S5atVDIuR4GU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_player_full_3first(player_name):\n",
        "  #player_name is given with the full name (first & last in that order)\n",
        "  player_name = player_name.strip().lower()\n",
        "  player_name = player_name.replace(\"-\",' ').replace(\"'\",\" \").replace(\".\",\"\").replace(\"(\",\"\").replace(\")\",\"\")\n",
        "  return player_name.split(' ')[-1]+' '+player_name.split(' ')[0][:3]\n",
        "\n",
        "def clean_player_only_name_full(player_name):\n",
        "  #only applied to df_betting\n",
        "  #We will then merge with inclusion in the unique ID,\n",
        "  #assuming that the 2 last names cannot be playing at the same time if they are not the same players\n",
        "  player_name_ini=player_name\n",
        "  player_name = player_name.strip().lower()\n",
        "  player_name = player_name.replace(\"'\",\" \").replace(\".\",\"\").replace(\"(\",\"\").replace(\")\",\"\").replace(\". \",\"\")\n",
        "  last_name=' '.join(player_name.split(' ')[:-1])\n",
        "  return last_name\n",
        "\n",
        "\n",
        "df_fixed_table_clean['player_name_clean_3first']=df_fixed_table_clean['Player Name'].apply(lambda x:clean_player_full_3first(x))\n",
        "df_matchs_clean['winner_name_clean_3first']=df_matchs_clean['winner_name'].apply(lambda x:clean_player_full_3first(x))\n",
        "df_matchs_clean['loser_name_clean_3first']=df_matchs_clean['loser_name'].apply(lambda x:clean_player_full_3first(x))\n",
        "df_ranking_clean['player_name_clean_3first']=df_ranking_clean['player_name'].apply(lambda x:clean_player_full_3first(x))\n",
        "\n",
        "df_betting_clean['clean_player_only_name_full']=df_betting_clean['Winner'].apply(lambda x:clean_player_only_name_full(x))\n",
        "df_betting_clean['clean_player_only_name_full']=df_betting_clean['Loser'].apply(lambda x:clean_player_only_name_full(x))\n",
        "\n"
      ],
      "metadata": {
        "id": "KPnNegclvQFi"
      },
      "execution_count": 276,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion:\n",
        "- we should remove all games of pablo vivero gonzalez\tbecause the player_name is duplicated\n",
        "- we should merge table match on betting AND ranking because this is the one for which we have 2 mapping keys."
      ],
      "metadata": {
        "id": "kThMGI6hRdBj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean Tournament Names"
      ],
      "metadata": {
        "id": "OJAQV-rlR70Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A tournament is defined as a tournament_name + date. These changes have been done above (df betting & df tournaments). See function create_unique_tournament_id"
      ],
      "metadata": {
        "id": "-G0qzz_SowUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "add_file_github('data/clean_datasets/df_betting_clean.csv',df_betting_clean.to_csv(index=False))\n",
        "add_file_github('data/clean_datasets/df_ranking_clean.csv',df_ranking_clean.to_csv(index=False))\n",
        "add_file_github('data/clean_datasets/df_fixed_table_clean.csv',df_fixed_table_clean.to_csv(index=False))\n",
        "add_file_github('data/clean_datasets/df_matchs_clean.csv',df_matchs_clean.to_csv(index=False))\n",
        "add_file_github('data/clean_datasets/df_tournaments_clean.csv',df_tournaments_clean.to_csv(index=False))"
      ],
      "metadata": {
        "id": "0EULQZ9GEUfS"
      },
      "execution_count": 289,
      "outputs": []
    }
  ]
}
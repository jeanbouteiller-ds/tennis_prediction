{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeanbouteiller-ds/tennis_prediction/blob/main/functions_fixed_table.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gorg-TjzXeal",
        "outputId": "136d0e59-c6fd-49fa-98e4-a913be128bc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed requests\n",
            "Successfully installed PyGitHub\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_package(package_name):\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package_name])\n",
        "        print(f\"Successfully installed {package_name}\")\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(f\"Failed to install {package_name}\") # Install the PyGitHub library\n",
        "\n",
        "install_package(\"requests\")\n",
        "install_package(\"PyGitHub\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import random\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "import os\n",
        "from github import Github\n",
        "from io import StringIO\n"
      ],
      "metadata": {
        "id": "4uiY8VTA6kN2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Set Up Github Environment"
      ],
      "metadata": {
        "id": "t_8rkjbj_xFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace these with your GitHub credentials\n",
        "username = \"jeanbouteiller-ds\"\n",
        "\n",
        "github_url = \"https://raw.githubusercontent.com/jeanbouteiller-ds/tennis_prediction/main/password.txt\"\n",
        "response = requests.get(github_url)\n",
        "\n",
        "password= response.text.replace('\\n','')\n",
        "repository_name = \"tennis_prediction\"\n",
        "branch_name = \"main\"  # Replace with your branch name\n",
        "\n",
        "\n",
        "# Initialize the GitHub client\n",
        "g = Github(username, password)\n",
        "\n",
        "# Access the desired repository\n",
        "repo = g.get_user().get_repo(repository_name)\n",
        "\n",
        "\n",
        "def add_file_github(file_name,file_content):\n",
        "  try:\n",
        "      file = repo.get_contents(file_name, ref=branch_name)\n",
        "      sha = file.sha\n",
        "  except Exception as e:\n",
        "      sha = None\n",
        "\n",
        "  if sha:\n",
        "      repo.update_file(file_name, \"Update file\", file_content, sha, branch_name)\n",
        "      # print(f\"Updated {file_name} in {repository_name}\")\n",
        "  else:\n",
        "      repo.create_file(file_name, \"Create file\", file_content, branch_name)\n",
        "      # print(f\"Created {file_name} in {repository_name}\")\n",
        "\n",
        "def get_file_content(file_name):\n",
        "  file_content = repo.get_contents(file_name, ref=branch_name)\n",
        "  file_content = file_content.decoded_content.decode(\"utf-8\")\n",
        "  if file_name.split('.')[1]=='txt':\n",
        "    file_content=eval(file_content)\n",
        "    return(file_content)\n",
        "  if file_name.split('.')[1]=='csv':\n",
        "    file_content=pd.read_csv(StringIO(file_content))\n",
        "    return(file_content)"
      ],
      "metadata": {
        "id": "KnltnHuj_0Fy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ3tMiFua2jH"
      },
      "source": [
        "# Get the list of all players and their associated urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xdDpI62ia13V"
      },
      "outputs": [],
      "source": [
        "#get the list of all first days of week between 2 dates\n",
        "#needed since the url on the atp website is updated every monday\n",
        "def get_list_ranking_dates(start_date, end_date):\n",
        "    mondays = []\n",
        "    current_date = start_date\n",
        "    one_day = timedelta(days=1)\n",
        "    while current_date <= end_date:\n",
        "        if current_date.weekday() == 0:  # Monday has a weekday index of 0\n",
        "            mondays.append(current_date.strftime('%Y-%m-%d'))\n",
        "        current_date += one_day\n",
        "    return mondays\n",
        "\n",
        "#get the list of all urls in which we have the rankings\n",
        "def find_list_weekly_ranking_url(start_date, end_date):\n",
        "  list_ranking_urls=[]\n",
        "  #date should be in format YYYY-MM-DD and should be a string\n",
        "  list_dates=get_list_ranking_dates(start_date, end_date)\n",
        "\n",
        "  for date_ranking in list_dates:\n",
        "    list_ranking_urls.append('https://www.atptour.com/en/rankings/singles?rankRange=1-200&rankDate='+date_ranking)\n",
        "  return(list_ranking_urls)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "eHvcYNfZxX6T"
      },
      "outputs": [],
      "source": [
        "#for each url, we extract the spans with the url associated with all players\n",
        "def span_from_url(weekly_ranking_url,span_list):\n",
        "  headers = {\n",
        "      'User-Agent': 'Your User Agent String Here'+str(random.random())\n",
        "  }\n",
        "\n",
        "  # Send an HTTP GET request with headers\n",
        "  response = requests.get(weekly_ranking_url, headers=headers)\n",
        "\n",
        "  # Check if the request was successful (status code 200)\n",
        "  if response.status_code == 200:\n",
        "      # Get the HTML content from the response\n",
        "      html_code = response.text\n",
        "  else:\n",
        "      html_code=''\n",
        "\n",
        "  soup = BeautifulSoup(html_code, 'html.parser')\n",
        "  selected_span={}\n",
        "\n",
        "  for span_name in span_list:\n",
        "    selected_span[span_name] = soup.find_all('span', class_=span_name)\n",
        "\n",
        "  return (selected_span)\n",
        "\n",
        "#from the above spans, we extract all urls\n",
        "def players_from_url(weekly_ranking_url):\n",
        "\n",
        "  selected_span=span_from_url(weekly_ranking_url,['player-cell-wrapper'])['player-cell-wrapper']\n",
        "\n",
        "  list_urls=[]\n",
        "  list_players=[]\n",
        "\n",
        "  for span_element in selected_span:\n",
        "    a_element = span_element.find('a')\n",
        "\n",
        "      # Extract the href attribute\n",
        "    if a_element:\n",
        "      # print(a_element)\n",
        "      href = 'https://www.atptour.com'+a_element.get('href')\n",
        "      player_name=href.split('players')[1].split('/')[1]\n",
        "      list_players.append(player_name)\n",
        "      list_urls.append(href)\n",
        "    else:\n",
        "        print(\"No <a> element found within the <span>.\")\n",
        "  return(list_players,list_urls)\n",
        "\n",
        "#\n",
        "def update_names_and_urls(weekly_ranking_url,list_all_names,list_all_urls):\n",
        "  list_players_name_and_url=players_from_url(weekly_ranking_url)\n",
        "  set_names_week = set(list_players_name_and_url[0])\n",
        "  set_urls_week = set(list_players_name_and_url[1])\n",
        "  set_names_all = set(list_all_names)\n",
        "  set_urls_all = set(list_all_urls)\n",
        "\n",
        "  names_to_add=list(set_names_week-set_names_all)\n",
        "  urls_to_add=list(set_urls_week-set_urls_all)\n",
        "\n",
        "  return(list_all_names+names_to_add,\n",
        "         list_all_urls+urls_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1PCwNHwCbVHv"
      },
      "outputs": [],
      "source": [
        "def create_initial_list(start_date,end_date):\n",
        "  list_all_names=[]\n",
        "  list_all_urls=[]\n",
        "\n",
        "  # Define your start and end dates\n",
        "    # Change this to your desired end date\n",
        "\n",
        "  list_year=[]\n",
        "  for weekly_url_ranking in find_list_weekly_ranking_url(start_date, end_date):\n",
        "    if weekly_url_ranking.split('rankDate=')[1].split('-')[0] not in list_year:\n",
        "      list_year.append(weekly_url_ranking.split('rankDate=')[1].split('-')[0])\n",
        "      # print(list_year)\n",
        "    names_and_url_week=update_names_and_urls(weekly_url_ranking,list_all_names,list_all_urls)\n",
        "    list_all_names=names_and_url_week[0]\n",
        "    list_all_urls=names_and_url_week[1]\n",
        "\n",
        "  list_all_urls_final=[0 for k in range(len(list_all_urls))]\n",
        "  for k in range(len(list_all_names)):\n",
        "    player_name=list_all_names[k]\n",
        "    for player_url in list_all_urls:\n",
        "      if player_name==player_url.split('/players/')[1].split('/')[0]:\n",
        "        list_all_urls_final[k]=player_url\n",
        "\n",
        "  return(list_all_names,list_all_urls_final)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Players Data"
      ],
      "metadata": {
        "id": "4YLTQ9nAyZnC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "rslHQS1EXrKv"
      },
      "outputs": [],
      "source": [
        "def find_stats_from_span(url,span_names_list=['table-height-cm-wrapper','table-weight-lbs','table-birthday'],\n",
        "               feature_names=['height','weight (lbs)','Birthdate']):\n",
        "  dict_data={}\n",
        "  data_spans=span_from_url(url,span_names_list)\n",
        "  for k in range (len(feature_names)):\n",
        "    key=feature_names[k]\n",
        "    values=list(data_spans.values())[k][0].get_text()\n",
        "    dict_data[key]=values.replace(' ','').replace('\\r\\n','')\n",
        "  return dict_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_stats_from_div(url,div_dict={'Turned Pro':['table-big-label','table-big-value'],\n",
        "                                      'Plays':['table-label','table-value']}):\n",
        "  #div_dict should be a dict where the key is the label that we want to find in the html code\n",
        "  # and the values are the div that we want to search (for label and values)\n",
        "  dict_data={}\n",
        "  headers = {\n",
        "      'User-Agent': 'Your User Agent String Here'+str(random.random())\n",
        "  }\n",
        "\n",
        "  # Send an HTTP GET request with headers\n",
        "  response = requests.get(url, headers=headers)\n",
        "\n",
        "  # Check if the request was successful (status code 200)\n",
        "  if response.status_code == 200:\n",
        "      # Get the HTML content from the response\n",
        "      html_code = response.text\n",
        "  else:\n",
        "      html_code=''\n",
        "\n",
        "  soup = BeautifulSoup(html_code, 'html.parser')\n",
        "\n",
        "  for feature_name in div_dict.keys():\n",
        "\n",
        "    div_name=div_dict[feature_name][0]\n",
        "    div_value=div_dict[feature_name][1]\n",
        "\n",
        "    for div in soup.find_all('div', class_=div_name):\n",
        "      # print(div)\n",
        "      if feature_name in div.get_text():\n",
        "\n",
        "        dict_data[feature_name]=(div.find_next_sibling('div', class_=div_value).get_text()).replace(' ','').replace('\\r\\n','')\n",
        "        # if\n",
        "        # print(div.find_next_sibling('div', class_='table-big-value').get_text())\n",
        "\n",
        "  return (dict_data)\n",
        "\n",
        "\n",
        "def find_all_stats(url):\n",
        "  return({**find_stats_from_div(url),**find_stats_from_span(url)})"
      ],
      "metadata": {
        "id": "axoxENTs6eZ5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# def create_new_fixed_table(file_name,list_player_names,list_urls):\n",
        "#   df_player_fixed=pd.DataFrame()\n",
        "#   nb_error=0\n",
        "#   for player_k in range(len(list_player_names)):\n",
        "#     url=list_urls[player_k]\n",
        "#     player_name=list_player_names[player_k]\n",
        "#     try:\n",
        "#       df_player_fixed[player_name]=find_all_stats(url)\n",
        "#     except Exception as e:\n",
        "#       nb_error+=1\n",
        "\n",
        "#   csv_player_fixed=df_player_fixed.T.to_csv(index=True)\n",
        "\n",
        "#   add_file_github(file_name,csv_player_fixed)"
      ],
      "metadata": {
        "id": "qHKHEis671g8"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import concurrent.futures\n",
        "\n",
        "def fetch_player_stats(url, player_name):\n",
        "    try:\n",
        "        stats = find_all_stats(url)\n",
        "        return (player_name, stats)\n",
        "    except Exception as e:\n",
        "        return (player_name, None)\n",
        "\n",
        "def create_new_fixed_table(file_name, list_player_names, list_urls):\n",
        "    player_data = []\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        futures = [executor.submit(fetch_player_stats, url, player_name) for url, player_name in zip(list_urls, list_player_names)]\n",
        "\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            player_name, stats = future.result()\n",
        "            if stats is not None:\n",
        "                player_data.append((player_name, stats))\n",
        "\n",
        "    df_player_fixed = pd.DataFrame(player_data, columns=['Player Name', 'Stats'])\n",
        "\n",
        "    # You can save the DataFrame to a file if needed.\n",
        "    df_player_fixed.to_csv(file_name, index=False)\n",
        "\n",
        "    return df_player_fixed\n",
        "\n",
        "# Usage\n",
        "# df = create_new_fixed_table(\"output.csv\", list_player_names, list_urls)\n"
      ],
      "metadata": {
        "id": "wZpL9P6oamSO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1eKWlpyRPemghzmN8X1cYFW6MOJTJpUDm",
      "authorship_tag": "ABX9TyO2JSGPcOZKouYZ77dyZVed",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeanbouteiller-ds/tennis_prediction/blob/main/functions_fixed_table.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import sys"
      ],
      "metadata": {
        "id": "V1g-E21RT8CO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gorg-TjzXeal",
        "outputId": "7bc9b98c-ae15-46bb-986a-b62109093d41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed requests\n"
          ]
        }
      ],
      "source": [
        "def install_package(package_name):\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package_name])\n",
        "        print(f\"Successfully installed {package_name}\")\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(f\"Failed to install {package_name}\") # Install the PyGitHub library\n",
        "\n",
        "install_package(\"requests\")\n",
        "# install_package(\"PyGitHub\")\n",
        "# install_package(\"pandas\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import random\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "import os\n",
        "import io\n",
        "# from github import Github\n",
        "\n",
        "import importlib.util\n",
        "import nbformat\n",
        "# import pandas"
      ],
      "metadata": {
        "id": "4uiY8VTA6kN2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the URL of the nb1 notebook on GitHub\n",
        "github_functions_url = \"https://raw.githubusercontent.com/jeanbouteiller-ds/tennis_prediction/main/github_functions.ipynb\"\n",
        "scraping_functions_url = \"https://raw.githubusercontent.com/jeanbouteiller-ds/tennis_prediction/main/scraping_functions.ipynb\"\n",
        "\n",
        "for file_functions in [github_functions_url,scraping_functions_url]:\n",
        "  # Download the notebook as a raw .ipynb file\n",
        "  response = requests.get(file_functions)\n",
        "  if response.status_code == 200:\n",
        "    notebook_content = response.text\n",
        "\n",
        "    # Parse the notebook content\n",
        "    notebook = nbformat.reads(notebook_content, as_version=4)\n",
        "\n",
        "    # Now you can execute the cells in the notebook\n",
        "    for cell in notebook.cells:\n",
        "        if cell.cell_type == 'code':\n",
        "            exec(cell.source)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xT1sXj9qUHmC",
        "outputId": "6282a3a6-4d4a-4839-bac2-2051de10fd2d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed requests\n",
            "Successfully installed PyGitHub\n",
            "Successfully installed pandas\n",
            "Successfully installed requests\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ3tMiFua2jH"
      },
      "source": [
        "# Get the list of all players and their associated urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xdDpI62ia13V"
      },
      "outputs": [],
      "source": [
        "#get the list of all first days of week between 2 dates\n",
        "#needed since the url on the atp website is updated every monday\n",
        "def get_list_ranking_dates(start_date, end_date):\n",
        "    mondays = []\n",
        "    current_date = start_date\n",
        "    one_day = timedelta(days=1)\n",
        "    while current_date <= end_date:\n",
        "        if current_date.weekday() == 0:  # Monday has a weekday index of 0\n",
        "            mondays.append(current_date.strftime('%Y-%m-%d'))\n",
        "        current_date += one_day\n",
        "    return mondays\n",
        "\n",
        "#get the list of all urls in which we have the rankings\n",
        "def find_list_weekly_ranking_url(start_date, end_date,nb_players_ranking):\n",
        "  list_ranking_urls=[]\n",
        "  #date should be in format YYYY-MM-DD and should be a string\n",
        "  list_dates=get_list_ranking_dates(start_date, end_date)\n",
        "\n",
        "  for date_ranking in list_dates:\n",
        "    list_ranking_urls.append('https://www.atptour.com/en/rankings/singles?rankRange=1-'+str(nb_players_ranking)+'&rankDate='+date_ranking)\n",
        "  return(list_ranking_urls)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "eHvcYNfZxX6T"
      },
      "outputs": [],
      "source": [
        "#from the above spans, we extract all urls\n",
        "def players_from_url(weekly_ranking_url):\n",
        "\n",
        "  selected_span=html_elements_from_url(weekly_ranking_url,'span',['player-cell-wrapper'])['player-cell-wrapper']\n",
        "\n",
        "  list_urls=[]\n",
        "  list_players=[]\n",
        "\n",
        "  for span_element in selected_span:\n",
        "    a_element = span_element.find('a')\n",
        "\n",
        "      # Extract the href attribute\n",
        "    if a_element:\n",
        "      # print(a_element)\n",
        "      href = 'https://www.atptour.com'+a_element.get('href')\n",
        "      player_name=href.split('players')[1].split('/')[1]\n",
        "      list_players.append(player_name)\n",
        "      list_urls.append(href)\n",
        "    else:\n",
        "        print(\"No <a> element found within the <span>.\")\n",
        "  return(list_players,list_urls)\n",
        "\n",
        "#\n",
        "def update_names_and_urls(weekly_ranking_url,list_all_names,list_all_urls):\n",
        "  list_players_name_and_url=players_from_url(weekly_ranking_url)\n",
        "  set_names_week = set(list_players_name_and_url[0])\n",
        "  set_urls_week = set(list_players_name_and_url[1])\n",
        "  set_names_all = set(list_all_names)\n",
        "  set_urls_all = set(list_all_urls)\n",
        "\n",
        "  names_to_add=list(set_names_week-set_names_all)\n",
        "  urls_to_add=list(set_urls_week-set_urls_all)\n",
        "\n",
        "  return(list_all_names+names_to_add,\n",
        "         list_all_urls+urls_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1PCwNHwCbVHv"
      },
      "outputs": [],
      "source": [
        "def create_initial_list(start_date,end_date,nb_players_ranking=200):\n",
        "  list_all_names=[]\n",
        "  list_all_urls=[]\n",
        "\n",
        "  # Define your start and end dates\n",
        "    # Change this to your desired end date\n",
        "\n",
        "  list_year=[]\n",
        "  for weekly_url_ranking in find_list_weekly_ranking_url(start_date, end_date,nb_players_ranking):\n",
        "    if weekly_url_ranking.split('rankDate=')[1].split('-')[0] not in list_year:\n",
        "      list_year.append(weekly_url_ranking.split('rankDate=')[1].split('-')[0])\n",
        "      # print(list_year)\n",
        "    names_and_url_week=update_names_and_urls(weekly_url_ranking,list_all_names,list_all_urls)\n",
        "    list_all_names=names_and_url_week[0]\n",
        "    list_all_urls=names_and_url_week[1]\n",
        "\n",
        "  list_all_urls_final=[0 for k in range(len(list_all_urls))]\n",
        "  for k in range(len(list_all_names)):\n",
        "    player_name=list_all_names[k]\n",
        "    for player_url in list_all_urls:\n",
        "      if player_name==player_url.split('/players/')[1].split('/')[0]:\n",
        "        list_all_urls_final[k]=player_url\n",
        "\n",
        "  return(list_all_names,list_all_urls_final)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Players Data"
      ],
      "metadata": {
        "id": "4YLTQ9nAyZnC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rslHQS1EXrKv"
      },
      "outputs": [],
      "source": [
        "def find_stats_from_span(url,span_names_list=['table-height-cm-wrapper','table-weight-lbs','table-birthday'],\n",
        "               feature_names=['height','weight (lbs)','Birthdate']):\n",
        "  dict_data={}\n",
        "  data_spans=html_elements_from_url(url,'span',span_names_list)\n",
        "  for k in range (len(feature_names)):\n",
        "    key=feature_names[k]\n",
        "    values=list(data_spans.values())[k][0].get_text()\n",
        "    dict_data[key]=values.replace(' ','').replace('\\r\\n','')\n",
        "  return dict_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_stats_from_div(url,div_dict={'Turned Pro':['table-big-label','table-big-value'],\n",
        "                                      'Plays':['table-label','table-value']}):\n",
        "  #div_dict should be a dict where the key is the label that we want to find in the html code\n",
        "  # and the values are the div that we want to search (for label and values)\n",
        "  dict_data={}\n",
        "\n",
        "  soup = parse_html_from_url(url)\n",
        "\n",
        "  for feature_name in div_dict.keys():\n",
        "\n",
        "    div_name=div_dict[feature_name][0]\n",
        "    div_value=div_dict[feature_name][1]\n",
        "\n",
        "    for div in soup.find_all('div', class_=div_name):\n",
        "      # print(div)\n",
        "      if feature_name in div.get_text():\n",
        "\n",
        "        dict_data[feature_name]=(div.find_next_sibling('div', class_=div_value).get_text()).replace(' ','').replace('\\r\\n','')\n",
        "        # if\n",
        "        # print(div.find_next_sibling('div', class_='table-big-value').get_text())\n",
        "\n",
        "  return (dict_data)\n",
        "\n",
        "\n",
        "def find_all_stats(url):\n",
        "  return({**find_stats_from_div(url),**find_stats_from_span(url)})"
      ],
      "metadata": {
        "id": "axoxENTs6eZ5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import concurrent.futures\n",
        "\n",
        "def fetch_player_stats(url, player_name):\n",
        "    try:\n",
        "        stats = find_all_stats(url)\n",
        "        return (player_name, stats)\n",
        "    except Exception as e:\n",
        "        return (player_name, None)\n",
        "\n",
        "#the below function has been created by the chatgpt optimizer\n",
        "def create_new_fixed_table(file_name, list_player_names, list_urls,write_csv=False):\n",
        "    player_data = []\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        futures = [executor.submit(fetch_player_stats, url, player_name) for url, player_name in zip(list_urls, list_player_names)]\n",
        "\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            player_name, stats = future.result()\n",
        "            if stats is not None:\n",
        "                # Flatten the stats dictionary into separate columns\n",
        "                player_data.append({**{'Player Name': player_name}, **stats})\n",
        "\n",
        "    df_player_fixed = pd.DataFrame(player_data)\n",
        "\n",
        "    # You can save the DataFrame to a file if needed.\n",
        "    if write_csv==True:\n",
        "      df_player_fixed_csv=df_player_fixed.to_csv(index=False)\n",
        "      add_file_github('df_player_fixed.csv',df_player_fixed_csv)\n",
        "\n",
        "    return df_player_fixed\n"
      ],
      "metadata": {
        "id": "wZpL9P6oamSO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Match tables functions"
      ],
      "metadata": {
        "id": "5mqWlMoBhvbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_matchs(tournament_row):\n",
        "  \"\"\"\n",
        "  Takes a row of the df_tournament.csv and returns the different games\n",
        "  \"\"\"\n",
        "  list_matchs_tournament=[]\n",
        "  tournament_url_archive=\"https://www.atptour.com/en/scores/archive\"+tournament_row[\"url\"].split('tournaments')[1].split(\"overview\")[0]+tournament_row['tournament_date'].split('.')[0]+'/results'\n",
        "  tournament_url_current=\"https://www.atptour.com/en/scores/current\"+tournament_row[\"url\"].split('tournaments')[1].split(\"overview\")[0]+tournament_row['tournament_date'].split('.')[0]+'/results'\n",
        "  print(tournament_url_archive,tournament_url_current)\n",
        "  scrapping_tournament_archive=html_elements_from_url(tournament_url_archive,\"div\",['scores-results-content'])[\"scores-results-content\"]\n",
        "  scrapping_tournament_current=html_elements_from_url(tournament_url_current,\"div\",['scores-results-content'])[\"scores-results-content\"]\n",
        "  scrapping_tournament=''\n",
        "  if len(scrapping_tournament_archive)>0:\n",
        "    scrapping_tournament=scrapping_tournament_archive\n",
        "  if (len(scrapping_tournament_current))>0:\n",
        "    scrapping_tournament=scrapping_tournament_current\n",
        "  if len(scrapping_tournament)>0:\n",
        "    scrapping_tournament=scrapping_tournament[0]\n",
        "    all_round=scrapping_tournament.find_all(\"tbody\")\n",
        "    round_tracker=0\n",
        "    for round in all_round:\n",
        "      all_games_round=round.find_all(\"td\",\"day-table-name\")\n",
        "      round_name=scrapping_tournament.find_all(\"th\")[round_tracker]\n",
        "      for game_number in range(0,len(all_games_round),2):\n",
        "        game_dict={}\n",
        "        game_dict[\"tournament_name\"]=tournament_row[\"tournament_name\"]\n",
        "        game_dict[\"tournament_date\"]=tournament_row[\"tournament_date\"]\n",
        "        game_dict[\"round\"]=round_name.text\n",
        "        game_dict[\"winner_name\"]=all_games_round[game_number].text.strip()\n",
        "        game_dict[\"loser_name\"]=all_games_round[game_number+1].text.strip()\n",
        "        game_dict[\"score\"]=re.sub(r'\\s+','-',scrapping_tournament.find_all(\"td\",\"day-table-score\")[int(game_number/2)].text.strip().replace(\"\\r\\n\",''))\n",
        "        list_matchs_tournament.append(game_dict)\n",
        "      round_tracker+=1\n",
        "  else:\n",
        "    game_dict={}\n",
        "    game_dict[\"tournament_name\"]=tournament_row[\"tournament_name\"]\n",
        "    game_dict[\"tournament_date\"]=tournament_row[\"tournament_date\"]\n",
        "    list_matchs_tournament.append(game_dict)\n",
        "  return list_matchs_tournament"
      ],
      "metadata": {
        "id": "-SCDJNk7h8GT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_HkEDMY0l4jB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yk-RW_X7l4_w"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1eKWlpyRPemghzmN8X1cYFW6MOJTJpUDm",
      "authorship_tag": "ABX9TyMU7h31phj4CvkGqh3Cnxy5",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}